{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Similarity Search With Faiss \n",
    "\n",
    "In this work, we will focus on similarity search on vectors. Following steps will be followed:\n",
    "\n",
    "- Creating random sentences\n",
    "- Creating embeddings\n",
    "- Creating a local vector database from those embeddings\n",
    "- Semantic search with given embedded query (A direct eculidian search)\n",
    "- Using Faiss library and seeing the performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "from faker import Faker\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Sentences\n",
    "Let's generate random sentences with faker library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sentence generator\n",
    "fake = Faker()\n",
    "def generate_meaningful_sentence(min_words=5, max_words=15):\n",
    "    sentence = fake.sentence(nb_words=random.randint(min_words, max_words))\n",
    "    return sentence\n",
    "\n",
    "# Csv file writer\n",
    "def create_csv(filename, num_sentences=20000, min_words=8, max_words=15):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Sentences\"])\n",
    "        for _ in range(num_sentences):\n",
    "            sentence = generate_meaningful_sentence(min_words, max_words)\n",
    "            writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and write to csv file\n",
    "create_csv('docs/random_sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings\n",
    "\n",
    "Let's use openai \"text-embedding-ada-002\" library to create library.\n",
    "\n",
    "Using this library is not free, if you want a free version you can use an open source one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Less hair win focus government edge less knowl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Several should high size turn sound side autho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Above field cup trial door use challenge owner.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Now role player social before it good page for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Them everything very official eat audience wai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences\n",
       "0  Less hair win focus government edge less knowl...\n",
       "1  Several should high size turn sound side autho...\n",
       "2    Above field cup trial door use challenge owner.\n",
       "3  Now role player social before it good page for...\n",
       "4  Them everything very official eat audience wai..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import csv file\n",
    "data = pd.read_csv('docs/random_sentences.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings with OpenAI\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI()\n",
    "\n",
    "def create_embeddings(data):\n",
    "        embeddings = client.embeddings.create(model=\"text-embedding-ada-002\", input=list(data))\n",
    "        embeddings_array = np.array([x.embedding for x in embeddings.data], float)\n",
    "        return embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings with chunks using OpenAI\n",
    "def chunk_sentences_and_create_embeddings(data, chunk_size):\n",
    "    embedding_list = []\n",
    "    num_sentences = len(data)\n",
    "    for i in range(0, num_sentences, chunk_size):\n",
    "        chunk_data = data[i:i+chunk_size]\n",
    "        embeddings = create_embeddings(chunk_data)\n",
    "        embedding_list.append(embeddings)\n",
    "    return np.array(embedding_list, float)\n",
    "\n",
    "embedding_array = chunk_sentences_and_create_embeddings(data[\"Sentences\"], chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape array\n",
    "embedding_array = embedding_array.reshape(-1, 1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Embeddings to csv file\n",
    "pd.DataFrame(\n",
    "    data=embedding_array\n",
    ").to_csv('docs/embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "def load_embeddings(file_path):\n",
    "    embeddings_df = pd.read_csv(file_path)\n",
    "    embeddings_array = embeddings_df.to_numpy()\n",
    "    return embeddings_array\n",
    "\n",
    "embedding_array = load_embeddings(\"docs/embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search\n",
    "\n",
    "Semantic search is a technique used to retrieve information based on the meaning and context of the query rather than just keyword matching. It aims to understand the intent behind the query and provide relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query embedding\n",
    "def query_embedding(query):\n",
    "    query_embedding = client.embeddings.create(model=\"text-embedding-ada-002\", input=query)\n",
    "    return query_embedding.data[0].embedding\n",
    "\n",
    "query = \"I don't want to be here\"\n",
    "query_embedding = query_embedding(query)\n",
    "len(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat L2 Index (Direct Euclidean Search)\n",
    "\n",
    "The flat L2 index is a type of index used in similarity search algorithms that calculates the Euclidean distance between vectors to measure their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faiss Index\n",
    "dimension = 1536 # dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension) # Initialize the index\n",
    "index.add(embedding_array) # Add the data to the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# See if the index is trained\n",
    "print(index.is_trained)\n",
    "\n",
    "# Number of elements in the index\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17167 15240  9998  9215 14756]]\n"
     ]
    }
   ],
   "source": [
    "# Search for the nearest neighbors of the query embedding\n",
    "query_vector = np.array(query_embedding, float).reshape(1, -1)\n",
    "D, I = index.search(query_vector, 5)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent history image feel morning outside ever try interest want leave resource.\n",
      "Attention machine interest I visit bad now to despite present.\n",
      "Weight billion save cold situation front school operation option interest down class meeting goal not allow find difference different myself.\n",
      "Reality walk among by decide out look return.\n",
      "Not product seat rock evening myself good.\n"
     ]
    }
   ],
   "source": [
    "# Let's see the most related sentences\n",
    "for i in I[0]:\n",
    "    print(data.iloc[i][\"Sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite good matches. Especially the first sentence is directly related and the other ones seems to have some related meanings without having the same direct words in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding Up\n",
    "\n",
    "This was an exact search. What we did actually one by one, calculating exact ecludian distances between the matrixes, and list the smallest distance to largest. Because of the embeddings, the closest has the highest meaning similarity.\n",
    "But this is in the end, not a very fast process. When data becomes big, it will be so much harder to make this search. Also, it will have no meaning to calculate most of the matrixes (They will be too far away.)\n",
    "Instead, we can limit the search into a limited area. We can generate centroids and make the search around it. In this way, we will gain so much speed in search traded off with accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the index\n",
    "nlist = 100 # number of clusters\n",
    "k = 4 # number of nearest neighbors\n",
    "quantizer = faiss.IndexFlatL2(dimension) # the other index\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist) # the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the index is trained\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index is not trained yet. We need to train it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the index\n",
    "index.train(embedding_array)\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the nearest neighbors of the query embedding\n",
    "index.add(embedding_array)\n",
    "D, I = index.search(query_vector, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15240,  9998,  9215,  6322]], dtype=int64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default number of visited cells is default 1. We can increase it with the parameter \"index.nprobe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of visited cells\n",
    "index.nprobe = 10\n",
    "D, I = index.search(query_vector, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15240,  9998,  9215,  6322]], dtype=int64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is different from the exact search, It can be better with increasing nprobe. nlist and k. It's a tradeoff with the speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
